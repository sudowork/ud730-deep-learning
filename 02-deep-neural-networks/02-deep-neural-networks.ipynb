{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Number of Parameters\n",
    "\n",
    "How many parameters did the previous multinomial logistic regression classifier have? As a reminder, images were 28x28 pixels and there were 10 classes.\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\mbox{# Parameters} & = W_\\mbox{size} + b_\\mbox{size} \\\\\n",
    "    \\mbox{# Parameters} & = (\\mbox{# Features} \\cdot \\mbox{# Classes}) + \\mbox{# Classes} \\\\\n",
    "    \\mbox{# Parameters} & = ((\\mbox{Image Width} \\cdot \\mbox{Image Height}) \\cdot \\mbox{# Classes}) + \\mbox{# Classes} \\\\\n",
    "    7850 & = ((28 \\cdot 28) \\cdot 10) + 10 \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "### Limitations\n",
    "\n",
    "For a linear model in general, if you have $N$ inputs and $K$ outputs, then the number of parameters is:\n",
    "\n",
    "$$ (N + 1) K $$\n",
    "\n",
    "However, in practice, you may want to use many, many more parameters.\n",
    "\n",
    "In addition linear models are **linear** (suprise, surprise). Representing with that model is limited. Given two pre-defined inputs that interact multiplicatively $y = X_1 \\times X_2$, you would not be able to capture that with a linear model $y = X_1 + X_2$.\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "Although somewhat limited, linear models are nice because they are efficient. Graphics processing units (GPUs) are exceedingly good at performing linear operations, like matrix multiplications. They are relatively cheap and very fast.\n",
    "\n",
    "### Stability\n",
    "\n",
    "Numerically linear operations are very stable. Small changes in input can never yield big changes in the output when you have bounded parameters.\n",
    "\n",
    "$$ \\Delta y_{small} \\sim |W|_{bounded}\\Delta x_{small} $$\n",
    "\n",
    "Derivatives of a linear function is constant:\n",
    "\n",
    "$$ \\frac{\\Delta y}{\\Delta x} = W^\\mathsf{T} $$\n",
    "$$ \\frac{\\Delta y}{\\Delta W} = x^\\mathsf{T} $$\n",
    "\n",
    "### Best of Both Worlds\n",
    "\n",
    "Keep parameters in big linear functions, but want the model to be nonlinear. You can't just keep multiplying by linear functions because that's just equivalent to one big linear function:\n",
    "\n",
    "$$ y = W_1 W_2 W_3 x = Wx $$\n",
    "\n",
    "Instead, introduce **nonlinearities** (using the sigmoid function $\\sigma$ here as an example):\n",
    "\n",
    "$$ y = W_1 \\sigma( W_2 \\sigma( W_3 x ) ) \\neq Wx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Units (ReLUs)\n",
    "\n",
    "ReLUs are the simplest nonlinear function:\n",
    "\n",
    "$$ \\mbox{ReLU}(x) = \\max(x, 0) $$\n",
    "\n",
    "In other words, when $x \\geq 0$, the value of $y(x)=x$; otherwise, it is $0$. Represented as a piecewise function:\n",
    "\n",
    "$$\n",
    "\\mbox{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "    x & \\mbox{if } x \\gt 0 \\\\\n",
    "    0 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "![Rectified Linear Unit](relu.png)\n",
    "\n",
    "### Derivative of a ReLU\n",
    "\n",
    "$$\n",
    "\\frac{d\\mbox{ReLU}(x)}{dx} =\n",
    "\\begin{cases}\n",
    "    1 & \\mbox{if } x \\gt 0 \\\\\n",
    "    0 & otherwise\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network of ReLUs\n",
    "\n",
    "### Simple 2-Layer Neural Network\n",
    "\n",
    "Below describes a simple, 2-layer neural network. The function is now nonlinear thanks to the ReLU in the middle. There is also now a new hyperparameter tune: the number of ReLUs $H$.\n",
    "\n",
    "![Simple Nonlinear Model with one ReLU](relu-network.png)\n",
    "\n",
    "The first layer consists of a set of weights and biases applied to $x$, which is then passed through the ReLUs. The output of the ReLU layer is then fed to the next layer, known as a **hidden layer**.\n",
    "\n",
    "The second layer consists of a set of weights and biases applied to the intermediate outputs.\n",
    "\n",
    "Finally, the output of the second layer is followed by the softmax function to generate probabilities.\n",
    "\n",
    "![2-Layer Neural Network](nn-2layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chain Rule\n",
    "\n",
    "One motivation for building the network out of stacking simple operations (multiplications, sums, ReLUs) is that it makes the math very simple thanks to the chain rule.\n",
    "\n",
    "The chain rule states that the derivative of two functions that get composed $g(f(x))'$ can be computed as the product of the derivatives of the components.\n",
    "\n",
    "$$ g(f(x))' = g'(f(x)) \\cdot f'(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In order to compute the gradients, you can introduce new operations to the graph that feed backwards. The output of these operations can then be used to update the existing feed-forward parameters. This is typically done for you by deep learning libraries.\n",
    "\n",
    "One thing to keep in mind is that the CPU and memory requirements for backpropagation are typically *twice* as much as their forward-propagation counterparts.\n",
    "\n",
    "![Backpropagation](backpropagation.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
